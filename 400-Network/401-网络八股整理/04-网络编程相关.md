## socket 编程
### Socket 有几种？

> (1) 基于 TCP 的 Socket (2) 基于 UDP 的 Socket (3) 基于 RawIp 的 Socket (4) 基于链路层的 Socket

### TCP 的 socket 编程流程

![](pics/Pasted%20image%2020231221142040.png)

- 服务端和客户端初始化 `socket`，得到文件描述符；
- 服务端调用 `bind`，将 socket 绑定在指定的 IP 地址和端口;
- 服务端调用 `listen`，进行监听；
- 服务端调用 `accept`，等待客户端连接；
- 客户端调用 `connect`，向服务端的地址和端口发起连接请求；
- 服务端 `accept` 返回用于传输的 `socket` 的文件描述符；
- 客户端调用 `write` 写入数据；服务端调用 `read` 读取数据；
- 客户端断开连接时，会调用 `close`，那么服务端 `read` 读取数据的时候，就会读取到了 `EOF`，待处理完数据后，服务端调用 `close`，表示连接关闭。

这里需要注意的是，服务端调用 `accept` 时，连接成功了会返回一个已完成连接的 socket，后续用来传输数据。

所以，监听的 socket 和真正用来传送数据的 socket，是「两个」 socket，一个叫作**监听 socket**，一个叫作**已完成连接 socket**。

成功连接建立之后，双方开始通过 read 和 write 函数来读写数据，就像往一个文件流里面写东西一样。

### listen 时候参数 backlog 的意义？

Linux 内核中会维护两个队列：

- 半连接队列（SYN 队列）：接收到一个 SYN 建立连接请求，处于 SYN_RCVD 状态；
- 全连接队列（Accpet 队列）：已完成 TCP 三次握手过程，处于 ESTABLISHED 状态；

![ SYN 队列 与 Accpet 队列 ](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230542373.png)

```
int listen (int socketfd, int backlog)
```

- 参数一 socketfd 为 socketfd 文件描述符
- 参数二 backlog，这参数在历史版本有一定的变化

在早期 Linux 内核 backlog 是 SYN 队列大小，也就是未完成的队列大小。

在 Linux 内核 2.2 之后，backlog 变成 accept 队列，也就是已完成连接建立的队列长度，**所以现在通常认为 backlog 是 accept 队列。**

**但是上限值是内核参数 somaxconn 的大小，也就说 accpet 队列长度 = min(backlog, somaxconn)。**

想详细了解 TCP 半连接队列和全连接队列，可以看这篇：[TCP 半连接队列和全连接队列满了会发生什么？又该如何应对？](https://xiaolincoding.com/network/3_tcp/tcp_queue.html)

### accept 发生在三次握手的哪一步？

我们先看看客户端连接服务端时，发送了什么？

![socket 三次握手](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/%E7%BD%91%E7%BB%9C/socket%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B.drawio.png)

- 客户端的协议栈向服务端发送了 SYN 包，并告诉服务端当前发送序列号 client_isn，客户端进入 SYN_SENT 状态；
- 服务端的协议栈收到这个包之后，和客户端进行 ACK 应答，应答的值为 client_isn+1，表示对 SYN 包 client_isn 的确认，同时服务端也发送一个 SYN 包，告诉客户端当前我的发送序列号为 server_isn，服务端进入 SYN_RCVD 状态；
- 客户端协议栈收到 ACK 之后，使得应用程序从 `connect` 调用返回，表示客户端到服务端的单向连接建立成功，客户端的状态为 ESTABLISHED，同时客户端协议栈也会对服务端的 SYN 包进行应答，应答数据为 server_isn+1；
- ACK 应答包到达服务端后，服务端的 TCP 连接进入 ESTABLISHED 状态，同时服务端协议栈使得 `accept` 阻塞调用返回，这个时候服务端到客户端的单向连接也建立成功。至此，客户端与服务端两个方向的连接都建立成功。

从上面的描述过程，我们可以得知**客户端 connect 成功返回是在第二次握手，服务端 accept 成功返回是在三次握手成功之后。**

### 客户端调用 close 了，连接是断开的流程是什么？

我们看看客户端主动调用了 `close`，会发生什么？

![客户端调用 close 过程](https://cdn.xiaolincoding.com//mysql/other/format,png-20230309230538308.png)

- 客户端调用 `close`，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态；
- 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 `EOF` 到接收缓冲区中，应用程序可以通过 `read` 调用来感知这个 FIN 包。这个 `EOF` 会被**放在已排队等候的其他已接收的数据之后**，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；
- 接着，当处理完数据后，自然就会读到 `EOF`，于是也调用 `close` 关闭它的套接字，这会使得服务端发出一个 FIN 包，之后处于 LAST_ACK 状态；
- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；
- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；
- 客户端经过 `2MSL` 时间之后，也进入 CLOSE 状态；

### 没有 accept，能建立 TCP 连接吗？

答案：**可以的**。

accpet 系统调用并不参与 TCP 三次握手过程，它只是负责从 TCP 全连接队列取出一个已经建立连接的 socket，用户层通过 accpet 系统调用拿到了已经建立连接的 socket，就可以对该 socket 进行读写操作了。

![半连接队列与全连接队列](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/TCP-%E5%8D%8A%E8%BF%9E%E6%8E%A5%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5/3.jpg)

更想了解这个问题，可以参考这篇文章：[没有 accept，能建立 TCP 连接吗？(opens new window)](https://xiaolincoding.com/network/3_tcp/tcp_no_accpet.html)

### 没有 listen，能建立 TCP 连接吗？

答案：**可以的**。

客户端是可以自己连自己的形成连接（**TCP 自连接**），也可以两个客户端同时向对方发出请求建立连接（**TCP 同时打开**），这两个情况都有个共同点，就是**没有服务端参与，也就是没有 listen，就能 TCP 建立连接。**

更想了解这个问题，可以参考这篇文章：[服务端没有 listen，客户端发起连接建立，会发生什么？](https://xiaolincoding.com/network/3_tcp/tcp_no_listen.html)

## 五种 IO 模型

查看文章：[[../网络编程-博客/高性能网络编程(五)：一文读懂高性能网络编程中的IO模型|5种IO模型]]

## IO 多路复用

>这个部分感觉好像大家面经写的比较宽泛，都是直接问对 IO 多路复用的理解啥的

下面来自 gpt：

I/O 多路复用（Input/Output Multiplexing）是一种用于处理多个 I/O 操作的技术，允许单个进程同时监视多个文件描述符（sockets、文件、设备等）的就绪状态。这种技术使得一个进程能够在同一时间内有效地处理多个 I/O 操作，而不需要为每个操作创建一个独立的线程或进程。

在传统的 I/O 模型中，每个 I/O 操作都可能导致阻塞，因为程序会等待数据的到达或发送完成。而使用 I/O 多路复用，程序可以同时监听多个 I/O 操作，只有在有就绪的数据时才进行实际的读取或写入操作，从而提高了系统的效率。

常见的 I/O 多路复用的机制包括：select、poll、epoll

使用 I/O 多路复用的好处在于，它可以减少线程或进程的数量，降低系统开销，提高程序的并发性能。这对于需要处理大量并发连接的网络服务器是特别有用的。

参考：[解析 Golang 网络 IO 模型之 EPOLL - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/609629545)

下面回到计算机领域，在 linux 操作系统中，对 IO 多路复用的概念有着更加明确的定义：

- 多路：存在多个需要处理 io event 的 fd（linux 中，一切皆文件，所有事务均可抽象为一个文件句柄 file descriptor，简称 fd）
- 复用：复用一个 loop thread 同时为多个 fd 提供处理服务（线程 thread 是内核视角下的最小调度单位；多路复用通常为循环模型 loop model，因此称为 loop thread）

解释了概念，下面再对 IO 多路复用附加一些约定俗称的要求：

IO 多路复用中，loop thread 是提供服务的乙方；待处理 io event 的 fd 们是甲方. 本着顾客是上帝的原则，乙方有义务为甲方提供更优质的服务，这里的服务质量就体现在一句话：”随叫随到，别让老板等久了”.

## select poll epoll

相关问题：
- select 和 epoll 了解吗，在实现机制上有什么异同吗？
- epoll poll select 讲讲（从底层 linux 源码机制到对比我都详细讲了）

linux 内核提供了三种经典的多路复用技术：

![](https://pic1.zhimg.com/80/v2-9cdc3e0d1e5f30ae71bc154ecda17504_1440w.webp)

从上图中可以看到，各个技术之间通过单向箭头连接，因此是一个持续演化改进的过程，select 最通用，但是相对粗糙；而 epoll 则最精致，在性能上也有着最优越的表现.

poll 在 select 的基础之上做了改进，但治标不治本，优化得不够彻底. 我们核心还是来对比看看 select 和 epoll 之间的共性和差异：

### （1）select

- 一次可以处理多个 fd，体现多路. 但 fd 数量有限，最多 1024 个
- loop thread 通过 select 将一组 fd 提交到内核做监听
- 当 fd 中无 io event 就绪时，loop thread 会陷入阻塞
- 每当这组 fd 中有 io event 到达时，内核会唤醒 loop thread
- loop thread 无法精准感知到哪些 fd 就绪，需要遍历一轮 fd 列表，时间复杂度 O(N)
- 托付给内核的 fd 列表只具有一轮交互的时效. 新的轮次中，loop thread 需要重新将监听的 fd 列表再传递给内核一次

### （2）epoll

- 每次处理的 fd 数量无上限
- loop thread 通过 epoll_create 操作创建一个 epoll 池子
- loop thread 通过 epoll_ctl 每次将一个待监听的 fd 添加到 epoll 池中
- 每当 fd 列表中有 fd 就绪事件到达时，会唤醒 loop threa. 同时内核会将处于就绪态的 fd 直接告知 loop thread，无需额外遍历

综上所述，select 和 epoll 等多路复用操作利用了内核的能力，能在待监听 fd 中有 io event 到达时，将 loop thread 唤醒，避免无意义的主动轮询操作.

其中，epoll 相比于 select 的核心性能优势在于：

- loop thread 被唤醒时，能明确知道哪些 fd 需要处理，减少了一次额外遍历的操作，时间复杂度由 O(N) 优化到 O(1)
- epoll 通过将创建池子和添加 fd 两个操作解耦，实现了池中 fd 数据的复用，减少了用户态与内核态间的数据拷贝成本
